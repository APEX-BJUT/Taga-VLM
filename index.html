<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TagaVLM: Topology-Aware Global Action Reasoning for Vision-Language Navigation. An end-to-end framework that architecturally embeds topological structures into VLMs for robust navigation.">
  <meta name="keywords" content="TagaVLM, VLN, Vision-Language Navigation, Topological Map, VLM, STAR-Att, ICRA 2026, Embodied AI, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TagaVLM: Topology-Aware Global Action Reasoning for Vision-Language Navigation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <style>
    .method-name {
      color: #bf0303;
      font-weight: 700;
    }
    .accent { color: #bf0303; }
    .hero.teaser .hero-body {
      padding-top: 0;
      padding-bottom: 2rem;
    }

    /* Results table */
    .results-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.93rem;
      font-family: 'Noto Sans', sans-serif;
    }
    .results-table th, .results-table td {
      padding: 7px 14px;
      text-align: center;
      border-bottom: 1px solid #eaeaea;
    }
    .results-table thead th {
      background: #fafafa;
      font-weight: 600;
    }
    .results-table thead tr:last-child th {
      border-bottom: 2px solid #ccc;
    }
    .results-table tbody tr.sep-row td {
      border-top: 1.5px solid #bbb;
    }
    .best { color: #bf0303; font-weight: 700; }
    .second { color: #e88; font-style: italic; }
    .ours-row { background: #fdf5f5; }
    .method-left { text-align: left !important; font-weight: 500; }

    /* Contribution list */
    .contrib-list {
      list-style: none !important;
      padding-left: 0 !important;
      margin-left: 0 !important;
      counter-reset: contrib;
    }
    .contrib-list li {
      position: relative;
      padding-left: 2.4em;
      margin-bottom: 1em;
      line-height: 1.65;
    }
    .contrib-list li::before {
      counter-increment: contrib;
      content: counter(contrib);
      position: absolute;
      left: 0;
      top: 0.22em;
      width: 1.5em;
      height: 1.5em;
      border-radius: 50%;
      background: #bf0303;
      color: #fff;
      font-size: 0.82em;
      font-weight: 700;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    /* Metric cards */
    .metric-card {
      text-align: center;
      padding: 1.2rem 0.5rem;
    }
    .metric-card .metric-value {
      font-size: 2.4rem;
      font-weight: 800;
      color: #bf0303;
      line-height: 1.1;
      font-family: 'Google Sans', sans-serif;
    }
    .metric-card .metric-label {
      font-size: 0.78rem;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: #999;
      margin-bottom: 0.3rem;
    }
    .metric-card .metric-delta {
      font-size: 0.8rem;
      color: #aaa;
      margin-top: 0.15rem;
    }

    .figure-caption {
      text-align: center;
      font-size: 0.88em;
      color: #666;
      margin-top: 0.75rem;
      line-height: 1.5;
      max-width: 90%;
      margin-left: auto;
      margin-right: auto;
    }

    .section-alt { background: #fafafa; }

    /* Smooth image presentation */
    .paper-figure {
      width: 100%;
      border-radius: 6px;
    }
  </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/APEX-BJUT">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>


<!-- ===== Title ===== -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size: 2.5rem;">
            <span class="accent">TagaVLM</span>: <span class="accent">T</span>opology-<span class="accent">A</span>ware
            <span class="accent">G</span>lobal <span class="accent">A</span>ction Reasoning<br>
            for Vision-Language Navigation
          </h1>
          <div class="is-size-5 publication-authors" style="margin-top: 0.6rem;">
            <span class="author-block" style="color: #bf0303; font-weight: 600;">ICRA 2026</span>
          </div>
          <div class="is-size-5 publication-authors" style="margin-top: 1.2rem;">
            <span class="author-block">Jiaxing Liu<sup>1,*</sup>,</span>
            <span class="author-block">Zexi Zhang<sup>1,2,*</sup>,</span>
            <span class="author-block">Xiaoyan Li<sup>1,&dagger;</sup>,</span>
            <span class="author-block">Boyue Wang<sup>1</sup>,</span>
            <span class="author-block">Yongli Hu<sup>1</sup>,</span>
            <span class="author-block">Baocai Yin<sup>1</sup></span>
          </div>
          <div class="is-size-6 publication-authors" style="margin-top: 0.5rem;">
            <span class="author-block"><sup>1</sup>Beijing University of Technology,</span>
            <span class="author-block"><sup>2</sup>Imperial College London</span>
          </div>
          <div class="is-size-7 publication-authors" style="margin-top: 0.25rem; color: #999;">
            <span>* Equal contribution &nbsp;&middot;&nbsp; &dagger; Corresponding author</span>
          </div>

          <div class="column has-text-centered" style="margin-top: 1.2rem;">
            <div class="publication-links">
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/APEX-BJUT/Taga-VLM" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== Teaser: Fig 1 (Motivation) ===== -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/fig1.png" alt="Motivation of TagaVLM" class="paper-figure" style="max-width: 65%; display: block; margin: 0 auto;">
      <p class="figure-caption" style="margin-top: 1rem;">
        <strong>Motivation.</strong>
        Previous methods (c) employ a two-stage pipeline that converts visual observations to text, losing crucial visual information.
        Our <span class="method-name">TagaVLM</span> (b) is an end-to-end paradigm that preserves VLM pretraining knowledge
        while directly incorporating online topological map information, enabling global action decisions (a) with backtracking ability.
      </p>
    </div>
  </div>
</section>


<!-- ===== Abstract ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-Language Navigation (VLN) presents a unique challenge for Large Vision-Language Models (VLMs) due to their inherent architectural mismatch: VLMs are primarily pretrained on static, disembodied vision-language tasks, which fundamentally clash with the dynamic, embodied, and spatially-structured nature of navigation. Existing large-model-based methods often resort to converting rich visual and spatial information into text, forcing models to implicitly infer complex visual-topological relationships or limiting their global action capabilities.
          </p>
          <p>
            To bridge this gap, we propose <span class="method-name">TagaVLM</span>
            (<strong class="accent">T</strong>opology-<strong class="accent">A</strong>ware
            <strong class="accent">G</strong>lobal <strong class="accent">A</strong>ction reasoning),
            an end-to-end framework that explicitly injects topological structures into the VLM backbone.
            To introduce topological edge information, <strong>Spatial Topology Aware Residual Attention (STAR-Att)</strong>
            directly integrates it into the VLM's self-attention mechanism, enabling intrinsic spatial reasoning while preserving pretrained knowledge.
            To enhance topological node information, an <strong>Interleaved Navigation Prompt</strong> strengthens node-level visual-text alignment.
            Finally, with the embedded topological graph, the model is capable of <strong>global action reasoning</strong>, allowing for robust path correction.
          </p>
          <p>
            On the R2R benchmark, <span class="method-name">TagaVLM</span> achieves state-of-the-art performance among large-model-based methods, with a <strong>Success Rate of 51.09%</strong> and <strong>SPL of 47.18</strong> in unseen environments, outperforming prior work by <strong>3.39% in SR</strong> and <strong>9.08 in SPL</strong>. This demonstrates that, for embodied spatial reasoning, targeted architectural enhancements on smaller open-source VLMs can be more effective than brute-force model scaling.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== Contributions ===== -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Contributions</h2>
        <div class="content has-text-justified">
          <p>
            The core thesis of this work is that the gap between disembodied VLMs and embodied navigation can be most effectively closed not by scaling model parameters, but by injecting the right structural priors directly into the architecture. We contribute:
          </p>
          <ol class="contrib-list">
            <li>
              <strong>An end-to-end topology-aware VLN framework.</strong>
              <span class="method-name">TagaVLM</span> is the first to architecturally embed topological graph structures into a VLM backbone for vision-language navigation, bridging the disembodied&ndash;embodied gap without sacrificing pretrained knowledge.
            </li>
            <li>
              <strong>Two synergistic mechanisms for graph injection.</strong>
              The <em>Interleaved Navigation Prompt</em> (INP) structures the input sequence to mirror the graph's node layout, strengthening node-level visual-text alignment. The <em>Spatial Topology Aware Residual Attention</em> (STAR-Att) injects topological edge relationships directly into the self-attention layers as a learnable residual bias, enabling the model to reason over spatial structure while preserving its expressive capacity.
            </li>
            <li>
              <strong>Evidence that inductive bias matters more than scale.</strong>
              <span class="method-name">TagaVLM</span>-0.5B, with architecturally injected topological priors, achieves competitive results against proprietary models orders of magnitude larger, while the 7B version sets a new state of the art&mdash;demonstrating that proper structural design is a compelling alternative to brute-force scaling for embodied spatial reasoning.
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== Method Overview ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified" style="margin-bottom: 1.5rem;">
          <p>
            <span class="method-name">TagaVLM</span> consists of four tightly-coupled components.
            <strong>(1)</strong> An <em>online topological map</em> records the visual observations, node types, and pairwise distances as the agent explores.
            <strong>(2)</strong> The <em>Interleaved Navigation Prompt</em> inserts each node's visual tokens at the corresponding <code>&lt;image&gt;</code> placeholder in the textual prompt, so that visual features are contextually adjacent to their node descriptions.
            <strong>(3)</strong> <em>STAR-Att</em> replaces every self-attention layer in the LLM backbone, adding a per-head learnable bias derived from the topological distance matrix; closer nodes attend more strongly, even when their visual features differ.
            <strong>(4)</strong> A <em>global action space</em> over all observed candidate nodes enables the agent to select any reachable target&mdash;including backtracking&mdash;at every step.
          </p>
        </div>
        <img src="./static/images/framework.png" alt="TagaVLM Framework" class="paper-figure"
             style="box-shadow: 0 2px 16px rgba(0,0,0,0.08);">
        <p class="figure-caption">
          <strong>Architecture of TagaVLM.</strong>
          The observation encoder maps RGB images to visual tokens. These are interleaved with textual node descriptions to form the INP.
          The LLM backbone, augmented with STAR-Att, fuses semantic and spatial information to produce a global action decision.
        </p>
      </div>
    </div>
  </div>
</section>


<!-- ===== STAR-Att ===== -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">STAR-Att: Spatial Topology Aware Residual Attention</h2>
        <div class="content has-text-justified" style="margin-bottom: 1.5rem;">
          <p>
            Standard self-attention treats all token pairs equally, regardless of their spatial relationship in the environment.
            STAR-Att addresses this by expanding the node-level pairwise distance matrix to the token level and adding it as a <em>learnable, per-head residual bias</em> to the attention scores.
            Tokens belonging to spatially closer nodes receive higher attention&mdash;even when their visual features are dissimilar&mdash;while the residual design preserves the pretrained semantic knowledge of the original attention.
            This provides a flexible inductive prior rather than a rigid constraint, allowing each attention head to independently calibrate the strength of spatial reasoning.
          </p>
        </div>
        <div class="has-text-centered">
          <video autoplay muted loop playsinline width="100%"
                 style="border-radius: 8px; box-shadow: 0 2px 16px rgba(0,0,0,0.08);">
            <source src="./static/videos/STAR-Att.mp4" type="video/mp4">
          </video>
          <p class="figure-caption">
            Visualization of the STAR-Att mechanism: topological distance information is injected as a residual attention bias,
            enabling spatially-aware reasoning across the navigation graph.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== Case Study ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Qualitative Analysis: Path Correction via Global Action Reasoning</h2>
        <div class="content has-text-justified" style="margin-bottom: 1.5rem;">
          <p>
            A key advantage of <span class="method-name">TagaVLM</span> is its ability to recover from navigation errors.
            In the example below, the agent initially moves in an incorrect direction due to the absence of visible landmarks.
            At Step 2, it leverages its spatial-topological understanding to recognize the mismatch with the instruction,
            performs a global action to backtrack to Node 1, and proceeds to the correct candidate node.
            The remaining steps successfully follow the instruction landmarks
            (<em>black chairs</em> &rarr; turn right &rarr; <em>refrigerator</em>) until the agent reaches the target and issues a stop decision.
          </p>
        </div>
        <div class="has-text-centered">
          <video autoplay muted loop playsinline width="100%"
                 style="border-radius: 8px; box-shadow: 0 2px 16px rgba(0,0,0,0.08);">
            <source src="./static/videos/case-study-2.mp4" type="video/mp4">
          </video>
          <p class="figure-caption">
            <strong>Navigation case study.</strong>
            The agent detects an early incorrect decision, backtracks via global action reasoning,
            and successfully completes the navigation trajectory to the target destination.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== Quantitative Results ===== -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Quantitative Results on R2R</h2>
    <div class="content has-text-justified" style="margin-bottom: 1.5rem;">
      <p>
        We compare with both cross-modal-backbone methods and large-model-based methods on the R2R benchmark.
        <span class="method-name">TagaVLM</span> surpasses all prior large-model-based approaches across every metric on both seen and unseen environments.
        Notably, our <strong>0.5B model already exceeds most large-model methods</strong>&mdash;including those built on proprietary GPT-4/GPT-4V&mdash;demonstrating
        that the right architectural priors can compensate for orders-of-magnitude differences in model scale.
      </p>
    </div>

    <!-- Main comparison table -->
    <div style="overflow-x: auto;">
      <table class="results-table">
        <thead>
          <tr>
            <th rowspan="2" class="method-left">Method</th>
            <th rowspan="2">Backbone</th>
            <th colspan="5" style="border-bottom: 1px solid #ddd;">Val Seen</th>
            <th colspan="5" style="border-bottom: 1px solid #ddd;">Val Unseen</th>
          </tr>
          <tr>
            <th>TL</th><th>NE&darr;</th><th>OSR&uarr;</th><th>SR&uarr;</th><th>SPL&uarr;</th>
            <th>TL</th><th>NE&darr;</th><th>OSR&uarr;</th><th>SR&uarr;</th><th>SPL&uarr;</th>
          </tr>
        </thead>
        <tbody>
          <!-- Cross-modal methods -->
          <tr>
            <td class="method-left" style="color:#888;">HAMT</td>
            <td style="color:#888;">Cross-Modal Trans.</td>
            <td>11.15</td><td>2.52</td><td>&ndash;</td><td>76</td><td>72</td>
            <td>11.46</td><td>2.29</td><td>&ndash;</td><td>66</td><td>61</td>
          </tr>
          <tr>
            <td class="method-left" style="color:#888;">DUET</td>
            <td style="color:#888;">Cross-Modal Trans.</td>
            <td>12.32</td><td>2.28</td><td>86</td><td>79</td><td>73</td>
            <td>13.94</td><td>3.31</td><td>81</td><td>72</td><td>60</td>
          </tr>
          <tr>
            <td class="method-left" style="color:#888;">BEVBert</td>
            <td style="color:#888;">Cross-Modal Trans.</td>
            <td>13.56</td><td>1.67</td><td>88</td><td>81</td><td>74</td>
            <td>14.55</td><td>2.81</td><td>84</td><td>75</td><td>64</td>
          </tr>
          <tr style="border-bottom: 2px solid #ccc;">
            <td class="method-left" style="color:#888;">ScaleVLN</td>
            <td style="color:#888;">Cross-Modal Trans.</td>
            <td>13.24</td><td>2.12</td><td>87</td><td>81</td><td>75</td>
            <td>14.09</td><td>2.09</td><td>88</td><td>81</td><td>70</td>
          </tr>
          <!-- Large model methods -->
          <tr class="sep-row">
            <td class="method-left">NavGPT</td>
            <td>GPT-4*</td>
            <td>&ndash;</td><td>&ndash;</td><td>&ndash;</td><td>&ndash;</td><td>&ndash;</td>
            <td>11.45</td><td>6.46</td><td>42</td><td>34</td><td>29</td>
          </tr>
          <tr>
            <td class="method-left">LangNav</td>
            <td>LLaMA2 (7B)</td>
            <td>&ndash;</td><td>7.4</td><td>40</td><td>32</td><td>28</td>
            <td>&ndash;</td><td>7.1</td><td>45</td><td>34</td><td>29</td>
          </tr>
          <tr>
            <td class="method-left">DiscussNav</td>
            <td>GPT-4*</td>
            <td>&ndash;</td><td>&ndash;</td><td>&ndash;</td><td>&ndash;</td><td>&ndash;</td>
            <td>9.69</td><td><span class="second">5.32</span></td><td>43</td><td>36.40</td><td>40</td>
          </tr>
          <tr>
            <td class="method-left">NavCoT</td>
            <td>LLaMA2 (7B)</td>
            <td>10.08</td><td>6.46</td><td>48.38</td><td>41.33</td><td>38.43</td>
            <td>9.95</td><td>6.26</td><td>48.11</td><td>40.23</td><td>36.64</td>
          </tr>
          <tr>
            <td class="method-left">MapGPT</td>
            <td>GPT-4V*</td>
            <td>&ndash;</td><td>&ndash;</td><td>&ndash;</td><td>&ndash;</td><td>&ndash;</td>
            <td>&ndash;</td><td>5.62</td><td><span class="second">57.9</span></td><td><span class="second">47.7</span></td><td>38.1</td>
          </tr>
          <!-- Ours -->
          <tr class="ours-row sep-row">
            <td class="method-left"><strong class="accent">TagaVLM (ours)</strong></td>
            <td><strong>Qwen2 (0.5B)</strong></td>
            <td>10.08</td><td><span class="second">5.23</span></td><td><span class="second">60.03</span></td><td><span class="second">53.48</span></td><td><span class="second">50.4</span></td>
            <td>9.8</td><td>5.57</td><td>55.09</td><td>45.72</td><td><span class="second">41.91</span></td>
          </tr>
          <tr class="ours-row">
            <td class="method-left"><strong class="accent">TagaVLM (ours)</strong></td>
            <td><strong>Qwen2 (7B)</strong></td>
            <td>10.16</td><td><span class="best">4.71</span></td><td><span class="best">64.15</span></td><td><span class="best">55.53</span></td><td><span class="best">53.05</span></td>
            <td>9.7</td><td><span class="best">4.97</span></td><td><span class="best">60.2</span></td><td><span class="best">51.09</span></td><td><span class="best">47.18</span></td>
          </tr>
        </tbody>
      </table>
    </div>
    <p style="text-align: center; font-size: 0.82rem; color: #999; margin-top: 0.6rem;">
      * Proprietary models accessed via black-box API. Cross-modal methods (gray) are shown for reference; our comparison target is the large-model category.
    </p>

    <!-- Key metrics -->
    <div class="columns is-centered" style="margin-top: 2.5rem;">
      <div class="column is-one-third">
        <div class="metric-card">
          <div class="metric-label">Success Rate (Unseen)</div>
          <div class="metric-value">51.09%</div>
          <div class="metric-delta">+3.39% vs. MapGPT</div>
        </div>
      </div>
      <div class="column is-one-third">
        <div class="metric-card">
          <div class="metric-label">SPL (Unseen)</div>
          <div class="metric-value">47.18</div>
          <div class="metric-delta">+9.08 vs. MapGPT</div>
        </div>
      </div>
      <div class="column is-one-third">
        <div class="metric-card">
          <div class="metric-label">Smallest Effective Model</div>
          <div class="metric-value">0.5B</div>
          <div class="metric-delta">Outperforms GPT-4 based methods</div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== Ablation Highlights ===== -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Ablation Highlights</h2>
    <div class="content has-text-justified" style="margin-bottom: 1.5rem;">
      <p>
        We systematically ablate each component on TagaVLM-0.5B (val unseen). Key findings:
      </p>
    </div>

    <div style="overflow-x: auto;">
      <table class="results-table">
        <thead>
          <tr>
            <th></th><th>STAR-Att</th><th>INP</th><th>Global Action</th><th>Aug. Data</th>
            <th>NE&darr;</th><th>OSR&uarr;</th><th>SR&uarr;</th><th>SPL&uarr;</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>(a)</td><td>&cross;</td><td>&cross;</td><td>&cross;</td><td>&cross;</td>
            <td>9.05</td><td>27.37</td><td>17.28</td><td>13.01</td>
          </tr>
          <tr>
            <td>(b)</td><td>&check;</td><td>&cross;</td><td>&cross;</td><td>&cross;</td>
            <td>7.74</td><td>35.67</td><td>26.14</td><td>20.81</td>
          </tr>
          <tr>
            <td>(c)</td><td>&check;</td><td>&check;</td><td>&cross;</td><td>&cross;</td>
            <td>6.49</td><td>47.47</td><td>38.40</td><td>35.61</td>
          </tr>
          <tr>
            <td>(e)</td><td>&check;</td><td>&check;</td><td>&check;</td><td>&cross;</td>
            <td>6.06</td><td>52.41</td><td>42.06</td><td>37.73</td>
          </tr>
          <tr class="ours-row" style="font-weight: 600;">
            <td>(f)</td><td>&check;</td><td>&check;</td><td>&check;</td><td>&check;</td>
            <td><span class="best">5.57</span></td><td><span class="best">55.09</span></td><td><span class="best">45.72</span></td><td><span class="best">41.91</span></td>
          </tr>
        </tbody>
      </table>
    </div>
    <div class="content has-text-justified" style="margin-top: 1.2rem;">
      <p>
        <strong>STAR-Att alone yields +8.86% SR</strong> over the vanilla fine-tuned baseline (a&rarr;b),
        confirming that explicit spatial priors are far more effective than relying on the model to learn topological relationships implicitly.
        Adding INP further improves SR by 12.26% (b&rarr;c), as the interleaved layout provides the contextual scaffolding
        that STAR-Att needs to apply spatial biases to the correct tokens.
        Global action reasoning contributes another +3.66% SR (c&rarr;e) through its backtracking capability.
        Together, the full system achieves <strong>28.44% absolute SR improvement</strong> over the baseline.
      </p>
    </div>

    <!-- STAR-Att vs Text-Based Map -->
    <h3 class="title is-5 has-text-centered" style="margin-top: 2rem;">STAR-Att vs. Text-Based Topological Map</h3>
    <div style="overflow-x: auto; max-width: 600px; margin: 0 auto;">
      <table class="results-table">
        <thead>
          <tr>
            <th></th><th>STAR-Att</th><th>Text-Based Map</th>
            <th>SR&uarr;</th><th>SPL&uarr;</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>(a)</td><td>&cross;</td><td>&cross;</td>
            <td>39.76</td><td>35.67</td>
          </tr>
          <tr>
            <td>(b)</td><td>&cross;</td><td>&check;</td>
            <td>40.70</td><td>36.92</td>
          </tr>
          <tr class="ours-row" style="font-weight: 600;">
            <td>(c)</td><td>&check;</td><td>&cross;</td>
            <td><span class="best">42.06</span></td><td><span class="best">37.73</span></td>
          </tr>
        </tbody>
      </table>
    </div>
    <p class="content has-text-centered" style="margin-top: 0.8rem; font-size: 0.92rem; color: #666;">
      Text-based topological descriptions (following MapGPT) improve SR by only +0.94%, while STAR-Att provides +2.30%&mdash;confirming that <em>architectural</em> injection of spatial priors is significantly more effective than <em>textual</em> prompting.
    </p>
  </div>
</section>


<!-- ===== Demo Video ===== -->
<section class="section section-alt">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demo Video</h2>
        <p class="content" style="margin-bottom: 1rem;">
          Real-time navigation of <span class="method-name">TagaVLM</span> in the Matterport3D simulator,
          demonstrating end-to-end instruction following, topological awareness, and path correction.
        </p>
        <div style="border-radius: 8px; overflow: hidden; box-shadow: 0 2px 16px rgba(0,0,0,0.08);">
          <video controls width="100%" playsinline preload="auto">
            <source src="./static/videos/demo.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- ===== BibTeX ===== -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{liu2026tagavlm,
  author    = {Liu, Jiaxing and Zhang, Zexi and Li, Xiaoyan and Wang, Boyue and Hu, Yongli and Yin, Baocai},
  title     = {TagaVLM: Topology-Aware Global Action Reasoning for Vision-Language Navigation},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2026},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/APEX-BJUT/Taga-VLM">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content" style="text-align: center;">
          <p>
            Website template adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
            licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
